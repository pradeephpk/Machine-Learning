{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural language processing (nlp)\n",
    "\n",
    "#Classifying ham or spam sms from smsspamcollection dataset\n",
    "\n",
    "#using natural language toolkit (nltk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('SMSSpamCollection',sep='\\t',names=['label','messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           messages\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def txt_clean(mess):\n",
    "    nopunc=[char for char in mess if char not in string.punctuation]\n",
    "    nopunc=''.join(nopunc)\n",
    "    \n",
    "    return[word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                          [Ok, lar, Joking, wif, u, oni]\n",
       "2       [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3           [U, dun, say, early, hor, U, c, already, say]\n",
       "4       [Nah, dont, think, goes, usf, lives, around, t...\n",
       "5       [FreeMsg, Hey, darling, 3, weeks, word, back, ...\n",
       "6       [Even, brother, like, speak, treat, like, aids...\n",
       "7       [per, request, Melle, Melle, Oru, Minnaminungi...\n",
       "8       [WINNER, valued, network, customer, selected, ...\n",
       "9       [mobile, 11, months, U, R, entitled, Update, l...\n",
       "10      [Im, gonna, home, soon, dont, want, talk, stuf...\n",
       "11      [SIX, chances, win, CASH, 100, 20000, pounds, ...\n",
       "12      [URGENT, 1, week, FREE, membership, £100000, P...\n",
       "13      [Ive, searching, right, words, thank, breather...\n",
       "14                                         [DATE, SUNDAY]\n",
       "15      [XXXMobileMovieClub, use, credit, click, WAP, ...\n",
       "16                                    [Oh, kim, watching]\n",
       "17      [Eh, u, remember, 2, spell, name, Yes, v, naug...\n",
       "18      [Fine, thats, way, u, feel, Thats, way, gota...\n",
       "19      [England, v, Macedonia, dont, miss, goalsteam,...\n",
       "20                               [seriously, spell, name]\n",
       "21           [I‘m, going, try, 2, months, ha, ha, joking]\n",
       "22                 [ü, pay, first, lar, da, stock, comin]\n",
       "23      [Aft, finish, lunch, go, str, lor, Ard, 3, smt...\n",
       "24               [Ffffffffff, Alright, way, meet, sooner]\n",
       "25      [forced, eat, slice, Im, really, hungry, tho, ...\n",
       "26                              [Lol, always, convincing]\n",
       "27      [catch, bus, frying, egg, make, tea, eating, m...\n",
       "28      [Im, back, amp, packing, car, Ill, let, know, ...\n",
       "29       [Ahhh, Work, vaguely, remember, feel, like, Lol]\n",
       "                              ...                        \n",
       "5542                    [Armand, says, get, ass, epsilon]\n",
       "5543          [U, still, havent, got, urself, jacket, ah]\n",
       "5544    [Im, taking, derek, amp, taylor, walmart, Im, ...\n",
       "5545                          [Hi, durban, still, number]\n",
       "5546                         [Ic, lotta, childporn, cars]\n",
       "5547    [contract, mobile, 11, Mnths, Latest, Motorola...\n",
       "5548                                 [trying, weekend, V]\n",
       "5549    [know, wot, people, wear, shirts, jumpers, hat...\n",
       "5550                             [Cool, time, think, get]\n",
       "5551            [Wen, get, spiritual, deep, Thats, great]\n",
       "5552    [safe, trip, Nigeria, Wish, happiness, soon, c...\n",
       "5553                             [Hahahause, brain, dear]\n",
       "5554    [Well, keep, mind, Ive, got, enough, gas, one,...\n",
       "5555    [Yeh, Indians, nice, Tho, kane, bit, shud, go,...\n",
       "5556          [Yes, thats, u, texted, Pshewmissing, much]\n",
       "5557    [meant, calculation, ltgt, units, ltgt, school...\n",
       "5558                            [Sorry, Ill, call, later]\n",
       "5559         [arent, next, ltgt, hours, imma, flip, shit]\n",
       "5560                        [Anything, lor, Juz, us, lor]\n",
       "5561    [Get, dump, heap, mom, decided, come, lowes, B...\n",
       "5562    [Ok, lor, Sony, ericsson, salesman, ask, shuhu...\n",
       "5563                             [Ard, 6, like, dat, lor]\n",
       "5564        [dont, wait, til, least, wednesday, see, get]\n",
       "5565                                           [Huh, lei]\n",
       "5566    [REMINDER, O2, get, 250, pounds, free, call, c...\n",
       "5567    [2nd, time, tried, 2, contact, u, U, £750, Pou...\n",
       "5568                   [ü, b, going, esplanade, fr, home]\n",
       "5569                     [Pity, mood, Soany, suggestions]\n",
       "5570    [guy, bitching, acted, like, id, interested, b...\n",
       "5571                                   [Rofl, true, name]\n",
       "Name: messages, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['messages'].apply(txt_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=Pipeline([\n",
    "    ('cov',CountVectorizer(analyzer=txt_clean)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('mnb',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(df['messages'],df['label'],test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cov', CountVectorizer(analyzer=<function txt_clean at 0x7f4473882730>, binary=False,\n",
       "        decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None,...inear_tf=False, use_idf=True)), ('mnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pipe.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       1.00      0.96      0.98      1543\n",
      "       spam       0.65      1.00      0.79       129\n",
      "\n",
      "avg / total       0.97      0.96      0.96      1672\n",
      "\n",
      "[[1475   68]\n",
      " [   0  129]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred,ytest))\n",
    "print(confusion_matrix(pred,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
